{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging,jieba,os,re\n",
    " \n",
    "def get_stopwords():\n",
    "    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',level=logging.INFO)\n",
    "    #加载停用词表\n",
    "    stopword_set = set()\n",
    "    with open(\"C:/Users/wsl/Desktop/kaikeba/xiangmu_1/dataset/NLP/stopwords.txt\",'r',encoding=\"utf-8\") as stopwords:\n",
    "        for stopword in stopwords:\n",
    "            stopword_set.add(stopword.strip(\"\\n\"))\n",
    "    return stopword_set\n",
    " \n",
    "'''\n",
    "使用正则表达式解析文本\n",
    "'''\n",
    "def parse_zhwiki(read_file_path,save_file_path):\n",
    "    #过滤掉<doc>\n",
    "    regex_str = \"[^<doc.*>$]|[^</doc>$]\"\n",
    "    file = open(read_file_path,\"r\",encoding=\"utf-8\")\n",
    "    #写文件\n",
    "    output = open(save_file_path,\"w+\",encoding=\"utf-8\")\n",
    "    content_line = file.readline()\n",
    "    #获取停用词表\n",
    "    stopwords = get_stopwords()\n",
    "     #定义一个字符串变量，表示一篇文章的分词结果\n",
    "    article_contents = \"\"\n",
    "    while content_line:\n",
    "        match_obj = re.match(regex_str,content_line)\n",
    "        content_line = content_line.strip(\"\\n\")\n",
    "        if len(content_line) > 0:\n",
    "            if match_obj:\n",
    "                #使用jieba进行分词\n",
    "                words = jieba.cut(content_line,cut_all=False)\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        article_contents += word+\" \"\n",
    "            else:\n",
    "                if len(article_contents) > 0:\n",
    "                    output.write(article_contents+\"\\n\")\n",
    "                    article_contents = \"\"\n",
    "        content_line = file.readline()\n",
    "    output.close()\n",
    " \n",
    "'''\n",
    "将维基百科语料库进行分类\n",
    "'''\n",
    "def generate_corpus():\n",
    "    zhwiki_path = \"C:/Users/wsl/Desktop/kaikeba/xiangmu_1/dataset/NLP/zhwiki/AA\"\n",
    "    save_path = \"C:/Users/wsl/Desktop/kaikeba/xiangmu_1/dataset/NLP/zhwiki/AA\"\n",
    "    for i in range(2):\n",
    "        file_path = os.path.join(zhwiki_path,str(\"wiki_0%s\"%str(i)))\n",
    "        parse_zhwiki(file_path,os.path.join(save_path,\"wiki_corpus0%s\"%str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2020-02-27 10:05:13,318:DEBUG:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wsl\\AppData\\Local\\Temp\\jieba.cache\n",
      "2020-02-27 10:05:13,369:DEBUG:Loading model from cache C:\\Users\\wsl\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.601 seconds.\n",
      "2020-02-27 10:05:14,924:DEBUG:Loading model cost 1.601 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2020-02-27 10:05:14,939:DEBUG:Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "generate_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "合并分词后的文件\n",
    "'''\n",
    "def merge_corpus():\n",
    "    output = open(\"C:/Users/wsl/Desktop/kaikeba/xiangmu_1/dataset/NLP/zhwiki/AA/wiki_corpus\",\"w\",encoding=\"utf-8\")\n",
    "    input = \"C:/Users/wsl/Desktop/kaikeba/xiangmu_1/dataset/NLP/zhwiki/AA\"\n",
    "    for i in range(2):\n",
    "        file_path = os.path.join(input,str(\"wiki_corpus0%s\"%str(i)))\n",
    "        file = open(file_path,\"r\",encoding=\"utf-8\")\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            output.writelines(line)\n",
    "            line = file.readline()\n",
    "        file.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
